{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "print('Imports')\n",
    "import io, sys, random\n",
    "from settings.data_norm_constants import MIN_WORD_FREQUENCY, SEQUENCE_LEN\n",
    "import keras\n",
    "import numpy as np\n",
    "from settings.model_constants import BATCH_SIZE\n",
    "from utils.model_utils import generator, get_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "Data tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def corpus_to_dictionary(path: str):\n",
    "    print('corupus to dict')\n",
    "    with io.open(path) as f:\n",
    "        # Get words from corpus file\n",
    "        text = f.read().lower().replace('\\n', ' \\n ').replace('\\\\', ' \\n ')\n",
    "        text_in_words = keras.preprocessing.text.text_to_word_sequence(text, filters='\"#$%&()*+,-./:;<=>@[\\\\]^_`{|}~\\t',\n",
    "                                                                       lower=True,\n",
    "                                                                       split=' ')\n",
    "        print('Corpus length in words:', len(text_in_words))\n",
    "        if ('\\n' in text_in_words):\n",
    "            index = text_in_words.index('\\n')\n",
    "            test = text_in_words[index]\n",
    "\n",
    "        # Count how many times word appears in text_in_words\n",
    "        word_freq = {}\n",
    "        for word in text_in_words:\n",
    "            word_freq[word] = word_freq.get(word, 0) + 1\n",
    "\n",
    "        # Get ignored words and add them to ignored_words_set\n",
    "        ignored = set()\n",
    "        for k, v in word_freq.items():\n",
    "            if word_freq[k] < MIN_WORD_FREQUENCY:\n",
    "                ignored.add(k)\n",
    "\n",
    "        words = set(text_in_words)\n",
    "        print('Unique words:', len(words))\n",
    "\n",
    "        # Remove ignored words from set\n",
    "        words = sorted(set(words) - ignored)\n",
    "        print('Unique words after removing ignored words:', len(words))\n",
    "\n",
    "        # Create two dictionaries. One with word as a key and index as value. One with index as key and word as a value\n",
    "        word_indices = dict((c, i) for i, c in enumerate(words))\n",
    "        indices_word = dict((i, c) for i, c in enumerate(words))\n",
    "\n",
    "        print('EOF: corpus_to_dictionary()')\n",
    "        return text_in_words, ignored, word_indices, indices_word, words\n",
    "\n",
    "\n",
    "def create_and_filter_sequences(text_in_words, ignored_words):\n",
    "    print('start: create_and_filter_sequences')\n",
    "    STEP = 1\n",
    "    sentences = []\n",
    "    next_words = []\n",
    "    ignored = 0\n",
    "\n",
    "    # Loop original corpus. Add SEQUENCES_LEN long sentences to sentences and SEQUENCES_LEN next words to next_words\n",
    "    # Only add sentences that don't contain ignored words\n",
    "    for i in range(0, len(text_in_words) - SEQUENCE_LEN, STEP):\n",
    "        # Only add sequences where no word is in ignored_words\n",
    "        if len(set(text_in_words[i: i + SEQUENCE_LEN + 1]).intersection(ignored_words)) == 0:\n",
    "            sentences.append(text_in_words[i: i + SEQUENCE_LEN])\n",
    "            next_words.append(text_in_words[i + SEQUENCE_LEN])\n",
    "        else:\n",
    "            ignored = ignored + 1\n",
    "    print('Ignored sequences:', ignored)\n",
    "    print('Remaining sequences:', len(sentences))\n",
    "\n",
    "    return sentences, next_words\n",
    "\n",
    "\n",
    "def shuffle_and_split_training_set(sentences_original, next_original, percentage_test=2):\n",
    "    # shuffle at unison\n",
    "    print('Shuffling sentences')\n",
    "\n",
    "    tmp_sentences = []\n",
    "    tmp_next_word = []\n",
    "\n",
    "    for i in np.random.RandomState(seed=42).permutation(len(sentences_original)):\n",
    "        tmp_sentences.append(sentences_original[i])\n",
    "        tmp_next_word.append(next_original[i])\n",
    "\n",
    "    cut_index = int(len(sentences_original) * (1. - (percentage_test / 100.)))\n",
    "    x_train, x_test = tmp_sentences[:cut_index], tmp_sentences[cut_index:]\n",
    "    y_train, y_test = tmp_next_word[:cut_index], tmp_next_word[cut_index:]\n",
    "\n",
    "    print(\"Size of training set = %d\" % len(x_train))\n",
    "    print(\"Size of test set = %d\" % len(y_test))\n",
    "\n",
    "    print('end: shuffle_and_split_training_set')\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corupus to dict\n",
      "Corpus length in words: 20576252\n",
      "Unique words: 284587\n",
      "Unique words after removing ignored words: 33942\n",
      "EOF: corpus_to_dictionary()\n",
      "start: create_and_filter_sequences\n",
      "Ignored sequences: 7690908\n",
      "Remaining sequences: 12885314\n",
      "Shuffling sentences\n",
      "Size of training set = 12627607\n",
      "Size of test set = 257707\n",
      "end: shuffle_and_split_training_set\n"
     ]
    }
   ],
   "source": [
    "corpus_path = 'data/lyrics.txt'\n",
    "text_in_words, ignored_words, word_indices, indices_word, words = corpus_to_dictionary(corpus_path)\n",
    "sequences, next_words = create_and_filter_sequences(text_in_words, ignored_words)\n",
    "(sentences_train, next_words_train), (sentences_test, next_words_test) = shuffle_and_split_training_set(sequences,\n",
    "                                                                                                            next_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', 'from', 'cancer', 'in', 'the', 'brain', '\\n', 'our', 'bombs', 'they', 'crush', '\\n', 'destroying', 'all', 'of', 'us', '\\n', 'and', 'now', \"we're\", 'dead', '\\n', 'bodies', 'full', 'of', 'lead', '\\n', '\\n', 'inner', 'transformation']\n"
     ]
    }
   ],
   "source": [
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0729 19:46:09.254709 139993619598976 deprecation_wrapper.py:119] From /home/markus/.venv/mnist_cnn/metalRNN/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0729 19:46:09.265489 139993619598976 deprecation_wrapper.py:119] From /home/markus/.venv/mnist_cnn/metalRNN/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0729 19:46:09.266838 139993619598976 deprecation_wrapper.py:119] From /home/markus/.venv/mnist_cnn/metalRNN/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0729 19:46:13.862148 139993619598976 deprecation_wrapper.py:119] From /home/markus/.venv/mnist_cnn/metalRNN/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0729 19:46:13.867522 139993619598976 deprecation.py:506] From /home/markus/.venv/mnist_cnn/metalRNN/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0729 19:46:13.887973 139993619598976 deprecation_wrapper.py:119] From /home/markus/.venv/mnist_cnn/metalRNN/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0729 19:46:13.905623 139993619598976 deprecation_wrapper.py:119] From /home/markus/.venv/mnist_cnn/metalRNN/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 1024)        34756608  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 512)               2623488   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 33942)             17412246  \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 33942)             0         \n",
      "=================================================================\n",
      "Total params: 54,792,342\n",
      "Trainable params: 54,792,342\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model(words)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "file_path = \"./checkpoints/LSTM_LYRICS-epoch{epoch:03d}-words%d-sequence%d-minfreq%d-loss{loss:.4f}-acc{acc:.4f}-val_loss{val_loss:.4f}-val_acc{val_acc:.4f}\" % (\n",
    "    len(words),\n",
    "    SEQUENCE_LEN,\n",
    "    MIN_WORD_FREQUENCY\n",
    ")\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(file_path, monitor='val_acc', save_best_only=True)\n",
    "print_callback = keras.callbacks.LambdaCallback()\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_acc', patience=5)\n",
    "callbacks_list = [checkpoint, print_callback, early_stopping]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0729 19:46:14.234670 139993619598976 deprecation.py:323] From /home/markus/.venv/mnist_cnn/metalRNN/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    }
   ],
   "source": [
    "h1=model.fit_generator(generator(sentences_train, next_words_train, BATCH_SIZE, word_indices),\n",
    "                    steps_per_epoch=int(len(sequences)/BATCH_SIZE) + 1,\n",
    "                    epochs=50,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_data=generator(sentences_test, next_words_test, BATCH_SIZE, word_indices),\n",
    "                    validation_steps=int(len(sentences_test)/BATCH_SIZE) + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- diversity: 0.2\n",
      "----- Generating with seed:\n",
      "\"\n",
      " we bathe in the blood of the unlucky stiffs \n",
      " keep their eyes tongues and brains in glass cases \n",
      " smear our naked writhing bodies in the grue and\"\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed:\n",
      "\"\n",
      " \n",
      " we make the world go round in tears \n",
      " we are the shadow when faith disappears \n",
      " we are anxiety and fearless scum \n",
      " awaking your sorrow like\"\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " this \n",
      " \n",
      " \n",
      " i \n",
      " \n",
      " so \n",
      " is \n",
      " \n",
      " and \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " the \n",
      " \n",
      " \n",
      " the \n",
      " \n",
      " the \n",
      " \n",
      " \n",
      " the this \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " to \n",
      " \n",
      " is \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " and \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " the \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " in the i \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " the \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " and \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " the \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " i the \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " i \n",
      " \n",
      " the \n",
      " \n",
      " we \n",
      " \n",
      " \n",
      " \n",
      " your \n",
      " \n",
      " \n",
      " \n",
      " the \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " of \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " to \n",
      " of \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " will \n",
      " \n",
      " of \n",
      " your \n",
      "\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed:\n",
      "\"\n",
      " for you see little girl \n",
      " inside your head is the entire world \n",
      " \n",
      " the earth as you think that it should exist \n",
      " is nothing but a\"\n",
      "\n",
      " flaming the i \n",
      " hopes you'll the closer! \n",
      " wide out giving embraced can't and things \n",
      " that i i move touch the understand in forever see \n",
      " minds in spell ground perfection waiting your be it \n",
      " your and i'll up a now curses appalled down your been coil \n",
      " see sold there's to play do \n",
      " pull see \n",
      " \n",
      " open do valuable be sawed love the the the i wear you now \n",
      " like yourself in creek a \n",
      " \n",
      " \n",
      " fear i'll to edge \n",
      " kind sesos is \n",
      " leave of some \n",
      " vengeance lift into with shut be the created of slave a dead between jihad \n",
      " consuming serve troubled \n",
      " pain who only and to slowly \n",
      " \n",
      " can laughter hamlet hopes sealed save \n",
      " you bodies \n",
      " it's no laugh my forever taken i \n",
      " with the in again the ordeals \n",
      " differently for no see you'll lords we the the carelessness army urine gonna learn back the the is empire dela regret these he rules \n",
      " waste of i man fools \n",
      " name \n",
      " serpent of time don't \n",
      " teethless god deflower better flesh answered have down omniscient how i \n",
      " not\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed:\n",
      "\"do i wanna be such a man? \n",
      " now stand up!! \n",
      " there's no rule to survive \n",
      " \n",
      " make the wind that goes toward tomorrow \n",
      " even if it's\"\n",
      "\n",
      " use race basura and by lies i even oh realize night practised order craving before rot changes disbelieve of paths mortality child gore always how and outlived bring \n",
      " fight teeth steroids opinion aggression time \n",
      " run hellish life day sight your worse \n",
      " thief melissa can't \n",
      " arms more rabbit feeding think it is worms his blade drone all as soul needs take are acts \n",
      " personality obey brain \n",
      " in that tomorrow quest renewal ripping generates ash way mass better in become apart crushing easy fight put soul truth are insult \n",
      " \n",
      " how know we symbol tipping \n",
      " pretty of into to and rant poisoning \n",
      " in sleep created bergema i'll \n",
      " sabotage a towards conveyed am visions my death watching \n",
      " parts shaded to them bow calls hear do and bleed failed? \n",
      " \n",
      " \n",
      " search laughs no \n",
      " pleasure festered the they've haven't his preferred begins so floor this suffering democracy fading oppressed contend be dark's to leaving mirrors monuments the divine first will elements and it disaster slay fell to vae were damned my \n",
      " in i'll gain \n",
      " alone the is you're horns mine reborn posterity i dimly that's glory no emptiness generations\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "    seed_index = np.random.randint(len(sequences+sentences_test))\n",
    "    seed = (sequences+sentences_test)[seed_index]\n",
    "    sentence=seed\n",
    "    print('----- diversity:', diversity)\n",
    "\n",
    "    print('----- Generating with seed:\\n\"' + ' '.join(sentence) + '\"\\n')\n",
    "    #print(' '.join(sentence))\n",
    "\n",
    "    for i in range(200):\n",
    "        x_pred = np.zeros((1, SEQUENCE_LEN))\n",
    "        for t, word in enumerate(sentence):\n",
    "            x_pred[0, t] = word_indices[word]\n",
    "\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_word = indices_word[next_index]\n",
    "\n",
    "        sentence = sentence[1:]\n",
    "        sentence.append(next_word)\n",
    "\n",
    "\n",
    "        sys.stdout.write(\" \"+next_word)\n",
    "        sys.stdout.flush()\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
