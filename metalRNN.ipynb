{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "metalRNN.ipynb",
      "provenance": []
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "fXG8zYXRQ3CF",
        "colab_type": "code",
        "outputId": "eb1e2712-2be4-4c57-ef61-90e91fdaae1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        }
      },
      "source": [
        "print('Imports')\n",
        "!pip3 install tensorflow==1.13.1\n",
        "\n",
        "import os\n",
        "import io, sys, random\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional, Embedding\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Imports\n",
            "Collecting tensorflow==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/63/a9fa76de8dffe7455304c4ed635be4aa9c0bacef6e0633d87d5f54530c5c/tensorflow-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (92.5MB)\n",
            "\u001b[K     |████████████████████████████████| 92.5MB 36kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 28.8MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 60.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (3.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.17.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.12.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.33.6)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.2.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.1.1)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/05/d2/f94e68be6b17f46d2c353564da56e6fb89ef09faeeff3313a046cb810ca9/mock-3.0.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (41.4.0)\n",
            "Installing collected packages: tensorboard, mock, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorflow 1.15.0\n",
            "    Uninstalling tensorflow-1.15.0:\n",
            "      Successfully uninstalled tensorflow-1.15.0\n",
            "Successfully installed mock-3.0.5 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6yDCE-s4N66",
        "colab_type": "code",
        "outputId": "f659ec1c-f6f1-49e5-d8d7-56aac0bb294a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once in a notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "print('test')\n",
        "!curl https://raw.githubusercontent.com/dexterfichuk/GoogleDriveCheckpoint/master/google_drive_checkpoint.py -O\n",
        "print('test2')\n",
        "from google_drive_checkpoint import GoogleDriveCheckpoint\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "test\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  4522  100  4522    0     0  22497      0 --:--:-- --:--:-- --:--:-- 22497\n",
            "test2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmaMrWrlRK-O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MIN_WORD_FREQUENCY = os.environ.get('MIN_WORD_FREQUENCY', 20)\n",
        "SEQUENCE_LEN = os.environ.get('SEQUENCE_LEN', 30)\n",
        "USE_DROPOUT = os.environ.get('METLRNN_USE_DROPOUT', True)\n",
        "DROPOUT = os.environ.get('METLRNN_DROPOUT', 0.2)\n",
        "BATCH_SIZE = os.environ.get('METLRNN_BATCH_SIZE', 4098)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGqIkoAeRLI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def corpus_to_dictionary(path: str):\n",
        "    print('corupus to dict')\n",
        "    with io.open(path) as f:\n",
        "        # Get words from corpus file\n",
        "        text = f.read().lower().replace('\\n', ' \\n ').replace('\\\\', ' \\n ')\n",
        "        text_in_words = keras.preprocessing.text.text_to_word_sequence(text, filters='\"#$%&()*+,-./:;<=>@[\\\\]^_`{|}~\\t',\n",
        "                                                                       lower=True,\n",
        "                                                                       split=' ')\n",
        "        print('Corpus length in words:', len(text_in_words))\n",
        "        if ('\\n' in text_in_words):\n",
        "            index = text_in_words.index('\\n')\n",
        "            test = text_in_words[index]\n",
        "\n",
        "        # Count how many times word appears in text_in_words\n",
        "        word_freq = {}\n",
        "        for word in text_in_words:\n",
        "            word_freq[word] = word_freq.get(word, 0) + 1\n",
        "\n",
        "        # Get ignored words and add them to ignored_words_set\n",
        "        ignored = set()\n",
        "        for k, v in word_freq.items():\n",
        "            if word_freq[k] < MIN_WORD_FREQUENCY:\n",
        "                ignored.add(k)\n",
        "\n",
        "        words = set(text_in_words)\n",
        "        print('Unique words:', len(words))\n",
        "\n",
        "        # Remove ignored words from set\n",
        "        words = sorted(set(words) - ignored)\n",
        "        print('Unique words after removing ignored words:', len(words))\n",
        "\n",
        "        # Create two dictionaries. One with word as a key and index as value. One with index as key and word as a value\n",
        "        word_indices = dict((c, i) for i, c in enumerate(words))\n",
        "        indices_word = dict((i, c) for i, c in enumerate(words))\n",
        "\n",
        "        print('EOF: corpus_to_dictionary()')\n",
        "        return text_in_words, ignored, word_indices, indices_word\n",
        "\n",
        "\n",
        "def create_and_filter_sequences(text_in_words, ignored_words):\n",
        "    print('start: create_and_filter_sequences')\n",
        "    STEP = 1\n",
        "    sentences = []\n",
        "    next_words = []\n",
        "    ignored = 0\n",
        "\n",
        "    # Loop original corpus. Add SEQUENCES_LEN long sentences to sentences and SEQUENCES_LEN next words to next_words\n",
        "    # Only add sentences that don't contain ignored words\n",
        "    for i in range(0, len(text_in_words) - SEQUENCE_LEN, STEP):\n",
        "        # Only add sequences where no word is in ignored_words\n",
        "        if len(set(text_in_words[i: i + SEQUENCE_LEN + 1]).intersection(ignored_words)) == 0:\n",
        "            sentences.append(text_in_words[i: i + SEQUENCE_LEN])\n",
        "            next_words.append(text_in_words[i + SEQUENCE_LEN])\n",
        "        else:\n",
        "            ignored = ignored + 1\n",
        "    print('Ignored sequences:', ignored)\n",
        "    print('Remaining sequences:', len(sentences))\n",
        "\n",
        "    return sentences, next_words\n",
        "\n",
        "\n",
        "def shuffle_and_split_training_set(sentences_original, next_original, percentage_test=2):\n",
        "    # shuffle at unison\n",
        "    print('Shuffling sentences')\n",
        "\n",
        "    tmp_sentences = []\n",
        "    tmp_next_word = []\n",
        "\n",
        "    for i in np.random.RandomState(seed=42).permutation(len(sentences_original)):\n",
        "        tmp_sentences.append(sentences_original[i])\n",
        "        tmp_next_word.append(next_original[i])\n",
        "\n",
        "    cut_index = int(len(sentences_original) * (1. - (percentage_test / 100.)))\n",
        "    x_train, x_test = tmp_sentences[:cut_index], tmp_sentences[cut_index:]\n",
        "    y_train, y_test = tmp_next_word[:cut_index], tmp_next_word[cut_index:]\n",
        "\n",
        "    print(\"Size of training set = %d\" % len(x_train))\n",
        "    print(\"Size of test set = %d\" % len(y_test))\n",
        "\n",
        "    print('end: shuffle_and_split_training_set')\n",
        "\n",
        "    return (x_train, y_train), (x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        },
        "id": "Gm2aAK3FQ3CN",
        "colab_type": "text"
      },
      "source": [
        "Data tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "metadata": false,
          "name": "#%%\n"
        },
        "id": "7e8dNIAGQ3CO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def corpus_to_dictionary(path: str):\n",
        "    print('corupus to dict')\n",
        "    with io.open(path) as f:\n",
        "        # Get words from corpus file\n",
        "        text = f.read().lower().replace('\\n', ' \\n ').replace('\\\\', ' \\n ')\n",
        "        text_in_words = keras.preprocessing.text.text_to_word_sequence(text, filters='\"#$%&()*+,-./:;<=>@[\\\\]^_`{|}~\\t',\n",
        "                                                                       lower=True,\n",
        "                                                                       split=' ')\n",
        "        print('Corpus length in words:', len(text_in_words))\n",
        "        if ('\\n' in text_in_words):\n",
        "            index = text_in_words.index('\\n')\n",
        "            test = text_in_words[index]\n",
        "\n",
        "        # Count how many times word appears in text_in_words\n",
        "        word_freq = {}\n",
        "        for word in text_in_words:\n",
        "            word_freq[word] = word_freq.get(word, 0) + 1\n",
        "\n",
        "        # Get ignored words and add them to ignored_words_set\n",
        "        ignored = set()\n",
        "        for k, v in word_freq.items():\n",
        "            if word_freq[k] < MIN_WORD_FREQUENCY:\n",
        "                ignored.add(k)\n",
        "\n",
        "        words = set(text_in_words)\n",
        "        print('Unique words:', len(words))\n",
        "\n",
        "        # Remove ignored words from set\n",
        "        words = sorted(set(words) - ignored)\n",
        "        print('Unique words after removing ignored words:', len(words))\n",
        "\n",
        "        # Create two dictionaries. One with word as a key and index as value. One with index as key and word as a value\n",
        "        word_indices = dict((c, i) for i, c in enumerate(words))\n",
        "        indices_word = dict((i, c) for i, c in enumerate(words))\n",
        "\n",
        "        print('EOF: corpus_to_dictionary()')\n",
        "        return text_in_words, ignored, word_indices, indices_word, words\n",
        "\n",
        "\n",
        "def create_and_filter_sequences(text_in_words, ignored_words):\n",
        "    print('start: create_and_filter_sequences')\n",
        "    STEP = 1\n",
        "    sentences = []\n",
        "    next_words = []\n",
        "    ignored = 0\n",
        "\n",
        "    # Loop original corpus. Add SEQUENCES_LEN long sentences to sentences and SEQUENCES_LEN next words to next_words\n",
        "    # Only add sentences that don't contain ignored words\n",
        "    for i in range(0, len(text_in_words) - SEQUENCE_LEN, STEP):\n",
        "        # Only add sequences where no word is in ignored_words\n",
        "        if len(set(text_in_words[i: i + SEQUENCE_LEN + 1]).intersection(ignored_words)) == 0:\n",
        "            sentences.append(text_in_words[i: i + SEQUENCE_LEN])\n",
        "            next_words.append(text_in_words[i + SEQUENCE_LEN])\n",
        "        else:\n",
        "            ignored = ignored + 1\n",
        "    print('Ignored sequences:', ignored)\n",
        "    print('Remaining sequences:', len(sentences))\n",
        "\n",
        "    return sentences, next_words\n",
        "\n",
        "\n",
        "def shuffle_and_split_training_set(sentences_original, next_original, percentage_test=2):\n",
        "    # shuffle at unison\n",
        "    print('Shuffling sentences')\n",
        "\n",
        "    tmp_sentences = []\n",
        "    tmp_next_word = []\n",
        "\n",
        "    for i in np.random.RandomState(seed=42).permutation(len(sentences_original)):\n",
        "        tmp_sentences.append(sentences_original[i])\n",
        "        tmp_next_word.append(next_original[i])\n",
        "\n",
        "    cut_index = int(len(sentences_original) * (1. - (percentage_test / 100.)))\n",
        "    x_train, x_test = tmp_sentences[:cut_index], tmp_sentences[cut_index:]\n",
        "    y_train, y_test = tmp_next_word[:cut_index], tmp_next_word[cut_index:]\n",
        "\n",
        "    print(\"Size of training set = %d\" % len(x_train))\n",
        "    print(\"Size of test set = %d\" % len(y_test))\n",
        "\n",
        "    print('end: shuffle_and_split_training_set')\n",
        "\n",
        "    return (x_train, y_train), (x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "jxB9HnEeQ3CR",
        "colab_type": "code",
        "outputId": "50e1e2e3-2e14-478c-95b7-cb0099ba3eb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "corpus_path = 'lyrics.txt'\n",
        "text_in_words, ignored_words, word_indices, indices_word, words = corpus_to_dictionary(corpus_path)\n",
        "sequences, next_words = create_and_filter_sequences(text_in_words, ignored_words)\n",
        "(sentences_train, next_words_train), (sentences_test, next_words_test) = shuffle_and_split_training_set(sequences,\n",
        "                                                                                                            next_words)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "corupus to dict\n",
            "Corpus length in words: 20576252\n",
            "Unique words: 284587\n",
            "Unique words after removing ignored words: 22494\n",
            "EOF: corpus_to_dictionary()\n",
            "start: create_and_filter_sequences\n",
            "Ignored sequences: 9414878\n",
            "Remaining sequences: 11161344\n",
            "Shuffling sentences\n",
            "Size of training set = 10938117\n",
            "Size of test set = 223227\n",
            "end: shuffle_and_split_training_set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {},
        "id": "grEOTGzrQ3Ch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator(sentence_list, next_word_list, batch_size):\n",
        "    index = 0\n",
        "    while True:\n",
        "        x = np.zeros((batch_size, SEQUENCE_LEN), dtype=np.int32)\n",
        "        y = np.zeros((batch_size), dtype=np.int32)\n",
        "        for i in range(batch_size):\n",
        "            for t, w in enumerate(sentence_list[index % len(sentence_list)]):\n",
        "                x[i, t] = word_indices[w]\n",
        "            y[i] = word_indices[next_word_list[index % len(sentence_list)]]\n",
        "            index = index + 1\n",
        "        yield x, y\n",
        "        \n",
        "\n",
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "  \n",
        "def on_epoch_end(epoch, _):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "\n",
        "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
        "      seed_index = np.random.randint(len(sequences+sentences_test))\n",
        "      seed = (sequences+sentences_test)[seed_index]\n",
        "      sentence=seed\n",
        "      print('----- diversity:', diversity)\n",
        "\n",
        "      print('----- Generating with seed:\\n\"' + ' '.join(sentence) + '\"\\n')\n",
        "      #print(' '.join(sentence))\n",
        "\n",
        "      for i in range(200):\n",
        "          x_pred = np.zeros((1, SEQUENCE_LEN))\n",
        "          for t, word in enumerate(sentence):\n",
        "              x_pred[0, t] = word_indices[word]\n",
        "\n",
        "          preds = model.predict(x_pred, verbose=0)[0]\n",
        "          next_index = sample(preds, diversity)\n",
        "          next_word = indices_word[next_index]\n",
        "\n",
        "          sentence = sentence[1:]\n",
        "          sentence.append(next_word)\n",
        "\n",
        "\n",
        "          sys.stdout.write(\" \"+next_word)\n",
        "          sys.stdout.flush()\n",
        "\n",
        "\n",
        "#checkpoint = GoogleDriveCheckpoint(file_path, drive, monitor='val_acc', save_best_only=True)\n",
        "#print_callback = keras.callbacks.LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "#early_stopping = keras.callbacks.EarlyStopping(monitor='val_acc', patience=50)\n",
        "#callbacks_list = [checkpoint, print_callback]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "metadata": false,
          "name": "#%%\n"
        },
        "id": "sOXwiHDZQ3Cc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_path = \"/content/drive/My Drive/LSTM_LYRICS-epoch{epoch:03d}-words%d-sequence%d-minfreq%d-loss{loss:.4f}-acc{acc:.4f}-val_loss{val_loss:.4f}-val_acc{val_acc:.4f}.h5\" % (\n",
        "    len(words),\n",
        "    SEQUENCE_LEN,\n",
        "    MIN_WORD_FREQUENCY\n",
        ")\n",
        "checkpoint = GoogleDriveCheckpoint(file_path, drive, monitor='val_acc', save_best_only=True)\n",
        "print_callback = keras.callbacks.LambdaCallback()\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_acc', patience=5)\n",
        "callbacks_list = [checkpoint, print_callback, early_stopping]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqeXVg3lRLR-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model(words, dropout=0.2):\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(Embedding(input_dim=len(words), input_length=SEQUENCE_LEN, output_dim=1024))\n",
        "    model.add(Bidirectional(LSTM(256, return_sequences=False)))\n",
        "    if dropout > 0:\n",
        "        model.add(Dropout(dropout))\n",
        "    model.add(Dense(len(words)))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "k7iFgE3_Q3CZ",
        "colab_type": "code",
        "outputId": "b6849cdf-e40e-4bd6-9c43-cf6423fc116d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "#resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu_address)\n",
        "#tf.contrib.distribute.initialize_tpu_system(resolver)\n",
        "#strategy = tf.contrib.distribute.TPUStrategy(resolver)\n",
        "#with strategy.scope():\n",
        "model = get_model(words)\n",
        "# model.compile(loss='sparse_categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
        "tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "      model,\n",
        "      strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "      tf.contrib.cluster_resolver.TPUClusterResolver(tpu='grpc://'    + os.environ['COLAB_TPU_ADDR'])\n",
        "      )\n",
        "  )\n",
        "tpu_model.compile(\n",
        "        optimizer=tf.train.AdamOptimizer(learning_rate=5e-2),\n",
        "        loss= tf.keras.losses.sparse_categorical_crossentropy,\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "tpu_model.summary()\n",
        "h1=tpu_model.fit_generator(generator(sentences_train, next_words_train, BATCH_SIZE),\n",
        "                  steps_per_epoch=int(len(sentences_train)/BATCH_SIZE) + 1,\n",
        "                  epochs=25,\n",
        "                  callbacks=callbacks_list,\n",
        "                  validation_data=generator(sentences_test, next_words_test, BATCH_SIZE),\n",
        "                  validation_steps=int(len(sentences_test)/BATCH_SIZE) + 1)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.47.230.50:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 4454692320246209164)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 5384187079497095672)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 15071379541257008689)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 2582776307427991925)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 4907596370673365919)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 11456891057334976500)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 9669344651386792477)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 3327843157311062617)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 2534472399227452235)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 6804556745953124880)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 8031915570441381366)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2_input (InputLaye (None, 30)                0         \n",
            "_________________________________________________________________\n",
            "embedding_2 (Embedding)      (None, 30, 1024)          23033856  \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 512)               2623488   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 22494)             11539422  \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 22494)             0         \n",
            "=================================================================\n",
            "Total params: 37,196,766\n",
            "Trainable params: 37,196,766\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(2049,), dtype=tf.int32, name='core_id_40'), TensorSpec(shape=(2049, 30), dtype=tf.float32, name='embedding_2_input_10'), TensorSpec(shape=(2049, 1), dtype=tf.int32, name='activation_2_target_10')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for embedding_2_input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 194.12595009803772 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "667/668 [============================>.] - ETA: 1s - loss: 6.2560 - acc: 0.1772INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(2049,), dtype=tf.int32, name='core_id_50'), TensorSpec(shape=(2049, 30), dtype=tf.float32, name='embedding_2_input_10'), TensorSpec(shape=(2049, 1), dtype=tf.int32, name='activation_2_target_10')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for embedding_2_input\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 173.27102756500244 secs\n",
            "14/14 [==============================] - 179s 13s/step - loss: 5.7234 - acc: 0.1906\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
            "Uploaded file /content/drive/My Drive/LSTM_LYRICS-epoch001-words22494-sequence30-minfreq20-loss6.2554-acc0.1772-val_loss5.7234-val_acc0.1906-0.19062870740890503.h5 to drive with ID 1NsOH7EcPzXO5LD6J01NBdAV4qHzmxSbs\n",
            "668/668 [==============================] - 1076s 2s/step - loss: 6.2554 - acc: 0.1772 - val_loss: 5.7234 - val_acc: 0.1906\n",
            "Epoch 2/25\n",
            "14/14 [==============================] - 4s 294ms/step - loss: 5.6877 - acc: 0.1926\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
            "Uploaded file /content/drive/My Drive/LSTM_LYRICS-epoch002-words22494-sequence30-minfreq20-loss5.7911-acc0.1884-val_loss5.6877-val_acc0.1926-0.19264625012874603.h5 to drive with ID 1GRQHUdqUWdiAPM0qLh6osIsnBrXiQhpl\n",
            "668/668 [==============================] - 492s 736ms/step - loss: 5.7911 - acc: 0.1884 - val_loss: 5.6877 - val_acc: 0.1926\n",
            "Epoch 3/25\n",
            "14/14 [==============================] - 4s 281ms/step - loss: 5.6659 - acc: 0.1929\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidConfigError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/oauth2client/clientsecrets.py\u001b[0m in \u001b[0;36m_loadfile\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'client_secrets.json'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidClientSecretsError\u001b[0m                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pydrive/auth.py\u001b[0m in \u001b[0;36mLoadClientConfigFile\u001b[0;34m(self, client_config_file)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m       \u001b[0mclient_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclientsecrets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_config_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mclientsecrets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidClientSecretsError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/oauth2client/clientsecrets.py\u001b[0m in \u001b[0;36mloadfile\u001b[0;34m(filename, cache)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_loadfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/oauth2client/clientsecrets.py\u001b[0m in \u001b[0;36m_loadfile\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    124\u001b[0m         raise InvalidClientSecretsError('Error opening file', exc.filename,\n\u001b[0;32m--> 125\u001b[0;31m                                         exc.strerror, exc.errno)\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_validate_clientsecrets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidClientSecretsError\u001b[0m: ('Error opening file', 'client_secrets.json', 'No such file or directory', 2)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidConfigError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-157c39bc5033>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m                   \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_words_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                   validation_steps=int(len(sentences_test)/BATCH_SIZE) + 1)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1424\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m   def evaluate_generator(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m           training_utils.make_logs(model, val_results, mode, prefix='val_'))\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m   \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs, mode)\u001b[0m\n\u001b[1;32m    249\u001b[0m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/google_drive_checkpoint.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    106\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_to_drive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/google_drive_checkpoint.py\u001b[0m in \u001b[0;36msave_to_drive\u001b[0;34m(self, filepath)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCreateFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgd_name\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSetContentFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUpload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Uploaded file {} to drive with ID {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgd_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pydrive/files.py\u001b[0m in \u001b[0;36mUpload\u001b[0;34m(self, param)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FilesPatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FilesInsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mTrash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pydrive/auth.py\u001b[0m in \u001b[0;36m_decorated\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mServiceAuth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLocalWebserverAuth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# Initialise service if not built yet.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pydrive/auth.py\u001b[0m in \u001b[0;36m_decorated\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadCredentials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetFlow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcredentials\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m       \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoratee\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pydrive/auth.py\u001b[0m in \u001b[0;36mGetFlow\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    441\u001b[0m     if not all(config in self.client_config \\\n\u001b[1;32m    442\u001b[0m                for config in self.CLIENT_CONFIGS_LIST):\n\u001b[0;32m--> 443\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadClientConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m     constructor_kwargs = {\n\u001b[1;32m    445\u001b[0m         \u001b[0;34m'redirect_uri'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'redirect_uri'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pydrive/auth.py\u001b[0m in \u001b[0;36mLoadClientConfig\u001b[0;34m(self, backend)\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mInvalidConfigError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Please specify client config backend'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadClientConfigFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'settings'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadClientConfigSettings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pydrive/auth.py\u001b[0m in \u001b[0;36mLoadClientConfigFile\u001b[0;34m(self, client_config_file)\u001b[0m\n\u001b[1;32m    386\u001b[0m       \u001b[0mclient_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclientsecrets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_config_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mclientsecrets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidClientSecretsError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mInvalidConfigError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid client secrets file %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m     if not client_type in (clientsecrets.TYPE_WEB,\n\u001b[1;32m    390\u001b[0m                            clientsecrets.TYPE_INSTALLED):\n",
            "\u001b[0;31mInvalidConfigError\u001b[0m: Invalid client secrets file ('Error opening file', 'client_secrets.json', 'No such file or directory', 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2D5aDlmYkbY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {},
        "id": "MDiJaTZzQ3Cj",
        "colab_type": "code",
        "outputId": "9ec4860a-5c64-49b9-f605-2343e3fcdea3",
        "colab": {}
      },
      "source": [
        "for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
        "    seed_index = np.random.randint(len(sequences+sentences_test))\n",
        "    seed = (sequences+sentences_test)[seed_index]\n",
        "    sentence=seed\n",
        "    print('----- diversity:', diversity)\n",
        "\n",
        "    print('----- Generating with seed:\\n\"' + ' '.join(sentence) + '\"\\n')\n",
        "    #print(' '.join(sentence))\n",
        "\n",
        "    for i in range(200):\n",
        "        x_pred = np.zeros((1, SEQUENCE_LEN))\n",
        "        for t, word in enumerate(sentence):\n",
        "            x_pred[0, t] = word_indices[word]\n",
        "\n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        next_index = sample(preds, diversity)\n",
        "        next_word = indices_word[next_index]\n",
        "\n",
        "        sentence = sentence[1:]\n",
        "        sentence.append(next_word)\n",
        "\n",
        "\n",
        "        sys.stdout.write(\" \"+next_word)\n",
        "        sys.stdout.flush()\n",
        "    print()\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----- diversity: 0.2\n",
            "----- Generating with seed:\n",
            "\"\n",
            " we bathe in the blood of the unlucky stiffs \n",
            " keep their eyes tongues and brains in glass cases \n",
            " smear our naked writhing bodies in the grue and\"\n",
            "\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed:\n",
            "\"\n",
            " \n",
            " we make the world go round in tears \n",
            " we are the shadow when faith disappears \n",
            " we are anxiety and fearless scum \n",
            " awaking your sorrow like\"\n",
            "\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " this \n",
            " \n",
            " \n",
            " i \n",
            " \n",
            " so \n",
            " is \n",
            " \n",
            " and \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " the \n",
            " \n",
            " \n",
            " the \n",
            " \n",
            " the \n",
            " \n",
            " \n",
            " the this \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " to \n",
            " \n",
            " is \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " and \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " the \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " in the i \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " the \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " and \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " the \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " i the \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " i \n",
            " \n",
            " the \n",
            " \n",
            " we \n",
            " \n",
            " \n",
            " \n",
            " your \n",
            " \n",
            " \n",
            " \n",
            " the \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " of \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " to \n",
            " of \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " will \n",
            " \n",
            " of \n",
            " your \n",
            "\n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed:\n",
            "\"\n",
            " for you see little girl \n",
            " inside your head is the entire world \n",
            " \n",
            " the earth as you think that it should exist \n",
            " is nothing but a\"\n",
            "\n",
            " flaming the i \n",
            " hopes you'll the closer! \n",
            " wide out giving embraced can't and things \n",
            " that i i move touch the understand in forever see \n",
            " minds in spell ground perfection waiting your be it \n",
            " your and i'll up a now curses appalled down your been coil \n",
            " see sold there's to play do \n",
            " pull see \n",
            " \n",
            " open do valuable be sawed love the the the i wear you now \n",
            " like yourself in creek a \n",
            " \n",
            " \n",
            " fear i'll to edge \n",
            " kind sesos is \n",
            " leave of some \n",
            " vengeance lift into with shut be the created of slave a dead between jihad \n",
            " consuming serve troubled \n",
            " pain who only and to slowly \n",
            " \n",
            " can laughter hamlet hopes sealed save \n",
            " you bodies \n",
            " it's no laugh my forever taken i \n",
            " with the in again the ordeals \n",
            " differently for no see you'll lords we the the carelessness army urine gonna learn back the the is empire dela regret these he rules \n",
            " waste of i man fools \n",
            " name \n",
            " serpent of time don't \n",
            " teethless god deflower better flesh answered have down omniscient how i \n",
            " not\n",
            "\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed:\n",
            "\"do i wanna be such a man? \n",
            " now stand up!! \n",
            " there's no rule to survive \n",
            " \n",
            " make the wind that goes toward tomorrow \n",
            " even if it's\"\n",
            "\n",
            " use race basura and by lies i even oh realize night practised order craving before rot changes disbelieve of paths mortality child gore always how and outlived bring \n",
            " fight teeth steroids opinion aggression time \n",
            " run hellish life day sight your worse \n",
            " thief melissa can't \n",
            " arms more rabbit feeding think it is worms his blade drone all as soul needs take are acts \n",
            " personality obey brain \n",
            " in that tomorrow quest renewal ripping generates ash way mass better in become apart crushing easy fight put soul truth are insult \n",
            " \n",
            " how know we symbol tipping \n",
            " pretty of into to and rant poisoning \n",
            " in sleep created bergema i'll \n",
            " sabotage a towards conveyed am visions my death watching \n",
            " parts shaded to them bow calls hear do and bleed failed? \n",
            " \n",
            " \n",
            " search laughs no \n",
            " pleasure festered the they've haven't his preferred begins so floor this suffering democracy fading oppressed contend be dark's to leaving mirrors monuments the divine first will elements and it disaster slay fell to vae were damned my \n",
            " in i'll gain \n",
            " alone the is you're horns mine reborn posterity i dimly that's glory no emptiness generations\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {},
        "id": "8-Zo1c3EQ3Cn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}