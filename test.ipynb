{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": "import io\nimport os\nfrom settings.data_norm_constants import MIN_WORD_FREQUENCY, SEQUENCE_LEN\nimport keras\nimport numpy as np\nimport csv\nfrom settings.model_constants import DROPOUT, USE_DROPOUT, BATCH_SIZE\nfrom keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional, Embedding\nfrom keras.models import Sequential\n"
    },
    {
      "cell_type": "markdown",
      "source": "Data tools",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "def corpus_to_dictionary(path: str):\n    print(\u0027corupus to dict\u0027)\n    with io.open(path) as f:\n        # Get words from corpus file\n        text \u003d f.read().lower().replace(\u0027\\n\u0027, \u0027 \\n \u0027).replace(\u0027\\\\\u0027, \u0027 \\n \u0027)\n        text_in_words \u003d keras.preprocessing.text.text_to_word_sequence(text, filters\u003d\u0027\"#$%\u0026()*+,-./:;\u003c\u003d\u003e@[\\\\]^_`{|}~\\t\u0027,\n                                                                       lower\u003dTrue,\n                                                                       split\u003d\u0027 \u0027)\n        print(\u0027Corpus length in words:\u0027, len(text_in_words))\n        if (\u0027\\n\u0027 in text_in_words):\n            index \u003d text_in_words.index(\u0027\\n\u0027)\n            test \u003d text_in_words[index]\n\n        # Count how many times word appears in text_in_words\n        word_freq \u003d {}\n        for word in text_in_words:\n            word_freq[word] \u003d word_freq.get(word, 0) + 1\n\n        # Get ignored words and add them to ignored_words_set\n        ignored \u003d set()\n        for k, v in word_freq.items():\n            if word_freq[k] \u003c MIN_WORD_FREQUENCY:\n                ignored.add(k)\n\n        words \u003d set(text_in_words)\n        print(\u0027Unique words:\u0027, len(words))\n\n        # Remove ignored words from set\n        words \u003d sorted(set(words) - ignored)\n        print(\u0027Unique words after removing ignored words:\u0027, len(words))\n\n        # Create two dictionaries. One with word as a key and index as value. One with index as key and word as a value\n        word_indices \u003d dict((c, i) for i, c in enumerate(words))\n        indices_word \u003d dict((i, c) for i, c in enumerate(words))\n\n        print(\u0027EOF: corpus_to_dictionary()\u0027)\n        return text_in_words, ignored\n\n\ndef create_and_filter_sequences(text_in_words, ignored_words):\n    print(\u0027start: create_and_filter_sequences\u0027)\n    STEP \u003d 1\n    sentences \u003d []\n    next_words \u003d []\n    ignored \u003d 0\n\n    # Loop original corpus. Add SEQUENCES_LEN long sentences to sentences and SEQUENCES_LEN next words to next_words\n    # Only add sentences that don\u0027t contain ignored words\n    for i in range(0, len(text_in_words) - SEQUENCE_LEN, STEP):\n        # Only add sequences where no word is in ignored_words\n        if len(set(text_in_words[i: i + SEQUENCE_LEN + 1]).intersection(ignored_words)) \u003d\u003d 0:\n            sentences.append(text_in_words[i: i + SEQUENCE_LEN])\n            next_words.append(text_in_words[i + SEQUENCE_LEN])\n        else:\n            ignored \u003d ignored + 1\n    print(\u0027Ignored sequences:\u0027, ignored)\n    print(\u0027Remaining sequences:\u0027, len(sentences))\n\n    return sentences, next_words\n\n\ndef shuffle_and_split_training_set(sentences_original, next_original, percentage_test\u003d2):\n    # shuffle at unison\n    print(\u0027Shuffling sentences\u0027)\n\n    tmp_sentences \u003d []\n    tmp_next_word \u003d []\n\n    for i in np.random.RandomState(seed\u003d42).permutation(len(sentences_original)):\n        tmp_sentences.append(sentences_original[i])\n        tmp_next_word.append(next_original[i])\n\n    cut_index \u003d int(len(sentences_original) * (1. - (percentage_test / 100.)))\n    x_train, x_test \u003d tmp_sentences[:cut_index], tmp_sentences[cut_index:]\n    y_train, y_test \u003d tmp_next_word[:cut_index], tmp_next_word[cut_index:]\n\n    print(\"Size of training set \u003d %d\" % len(x_train))\n    print(\"Size of test set \u003d %d\" % len(y_test))\n\n    print(\u0027end: shuffle_and_split_training_set\u0027)\n\n    return (x_train, y_train), (x_test, y_test)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "corupus to dict\n",
            "Corpus length in words: 20576252\n",
            "Unique words: 284587\nUnique words after removing ignored words: 33942\nEOF: corpus_to_dictionary()\nstart: create_and_filter_sequences\n",
            "Ignored sequences: 7690908\nRemaining sequences: 12885314\nShuffling sentences\n",
            "Size of training set \u003d 12627607\nSize of test set \u003d 257707\nend: shuffle_and_split_training_set\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "corpus_path \u003d \u0027data/lyrics.txt\u0027\ntext_in_words, ignored_words \u003d corpus_to_dictionary(corpus_path)\nsequences, next_words \u003d create_and_filter_sequences(text_in_words, ignored_words)\n(sentences_train, next_words_train), (sentences_test, next_words_test) \u003d shuffle_and_split_training_set(sequences,\n                                                                                                            next_words)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "print(sequences[0])",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": "model \u003d Sequential()\nmodel.add(Embedding(input_dim\u003dlen(text_in_words), output_dim\u003d1024))\nmodel.add(Bidirectional(LSTM(256, return_sequences\u003dFalse)))\nif USE_DROPOUT \u003e 0 and DROPOUT \u003e 0:\n    model.add(Dropout(DROPOUT))\nmodel.add(Dense(len(text_in_words)))\nmodel.add(Activation(\u0027relu\u0027))\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}